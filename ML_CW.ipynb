{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gkFoj9zX2SdGOphlL5a0AtGcBClPISoL",
      "authorship_tag": "ABX9TyMmrsJOEnxze+rrNMVBupoC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORT AND  INSTALLATION"
      ],
      "metadata": {
        "id": "teM4VczdWE8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install imbalanced-learn -q\n",
        "!pip install tensorflow -q\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix,\n",
        "                             classification_report, roc_curve)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import joblib\n",
        "import os\n",
        "from google.colab import drive, files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "metadata": {
        "id": "yirxqMdKWEMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA LOADING & INITIAL EXPLORATION"
      ],
      "metadata": {
        "id": "FOr8XlUyrifm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TELCO CUSTOMER CHURN PREDICTION - COURSEWORK IMPLEMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download dataset from Kaggle (alternative method)\n",
        "print(\"\\n1. DOWNLOADING DATASET...\")\n",
        "\n",
        "# Method 1: Try loading from Google Drive path first\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Telco-Customer-Churn.csv\")\n",
        "    print(f\"   Dataset loaded successfully from Google Drive!\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Columns: {df.columns.tolist()}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"   File not found at Google Drive path. Trying alternative method...\")\n",
        "\n",
        "    # Method 2: Upload from local\n",
        "    print(\"\\n   Please upload the 'Telco-Customer-Churn.csv' file when prompted\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the filename\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        print(f\"   Uploaded file: {filename}\")\n",
        "\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(filename)\n",
        "        print(f\"   Dataset loaded successfully from uploaded file!\")\n",
        "        print(f\"   Shape: {df.shape}\")\n",
        "        print(f\"   Columns: {df.columns.tolist()}\")\n",
        "    else:\n",
        "        print(\"    No file uploaded. Please try again.\")\n",
        "        raise FileNotFoundError(\"Telco-Customer-Churn.csv file not found\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n Data loading completed!\")"
      ],
      "metadata": {
        "id": "GhCIEW-Nr5Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXPLORATORY DATA ANALYSIS (EDA)"
      ],
      "metadata": {
        "id": "yo57YIxbsRlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS (EDA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define DRIVE_PATH for saving visualizations\n",
        "DRIVE_PATH = './colab_output'\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# 2.1 Basic information\n",
        "print(\"\\n2.1 DATASET INFORMATION:\")\n",
        "print(f\"   Total samples: {len(df)}\")\n",
        "print(f\"   Total features: {len(df.columns)}\")\n",
        "\n",
        "# Data types\n",
        "print(\"\\n2.2 DATA TYPES:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Missing values\n",
        "print(\"\\n2.3 MISSING VALUES:\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0])\n",
        "\n",
        "# Handle missing values in TotalCharges\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "# 2.4 Class Distribution - BEFORE BALANCING\n",
        "print(\"\\n2.4 CLASS DISTRIBUTION (BEFORE BALANCING):\")\n",
        "churn_counts = df['Churn'].value_counts()\n",
        "churn_percent = df['Churn'].value_counts(normalize=True) * 100\n",
        "print(f\"   No Churn:  {churn_counts['No']} ({churn_percent['No']:.1f}%)\")\n",
        "print(f\"   Churn:     {churn_counts['Yes']} ({churn_percent['Yes']:.1f}%)\")\n",
        "print(f\"   IMBALANCE RATIO: {churn_counts['No']/churn_counts['Yes']:.2f}:1\")\n",
        "\n",
        "# Save for report\n",
        "class_dist_before = {\n",
        "    'No': churn_counts['No'],\n",
        "    'Yes': churn_counts['Yes'],\n",
        "    'No_percent': churn_percent['No'],\n",
        "    'Yes_percent': churn_percent['Yes']\n",
        "}\n",
        "\n",
        "# 2.5 Numerical features statistics\n",
        "print(\"\\n2.5 NUMERICAL FEATURES STATISTICS:\")\n",
        "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "print(df[numerical_cols].describe())\n",
        "\n",
        "# 2.6 Visualizations\n",
        "print(\"\\n2.6 CREATING EDA VISUALIZATIONS...\")\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Subplot 1: Churn distribution\n",
        "plt.subplot(3, 3, 1)\n",
        "sns.countplot(data=df, x='Churn', palette='Set2')\n",
        "plt.title('Churn Distribution (Original)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Churn Status', fontsize=12)\n",
        "\n",
        "# Add percentage labels\n",
        "total = len(df)\n",
        "for p in plt.gca().patches:\n",
        "    height = p.get_height()\n",
        "    plt.gca().text(p.get_x() + p.get_width()/2., height + 30,\n",
        "                  f'{height/total*100:.1f}%', ha='center', fontsize=11)\n",
        "\n",
        "# Subplot 2: Tenure distribution\n",
        "plt.subplot(3, 3, 2)\n",
        "sns.histplot(df['tenure'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Customer Tenure Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Tenure (months)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "# Subplot 3: Monthly charges vs Churn\n",
        "plt.subplot(3, 3, 3)\n",
        "sns.boxplot(data=df, x='Churn', y='MonthlyCharges', palette='Set2')\n",
        "plt.title('Monthly Charges by Churn Status', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Churn Status', fontsize=12)\n",
        "plt.ylabel('Monthly Charges ($)', fontsize=12)\n",
        "\n",
        "# Subplot 4: Contract type impact\n",
        "plt.subplot(3, 3, 4)\n",
        "contract_churn = pd.crosstab(df['Contract'], df['Churn'], normalize='index') * 100\n",
        "contract_churn.plot(kind='bar', stacked=True, colormap='coolwarm', ax=plt.gca())\n",
        "plt.title('Churn Rate by Contract Type', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Contract Type', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 5: Correlation heatmap\n",
        "plt.subplot(3, 3, 5)\n",
        "corr_matrix = df[numerical_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.2f', cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Numerical Features Correlation', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Subplot 6: Payment method impact\n",
        "plt.subplot(3, 3, 6)\n",
        "payment_churn = pd.crosstab(df['PaymentMethod'], df['Churn'], normalize='index') * 100\n",
        "payment_churn.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())\n",
        "plt.title('Churn Rate by Payment Method', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Payment Method', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 7: Internet service impact\n",
        "plt.subplot(3, 3, 7)\n",
        "internet_churn = pd.crosstab(df['InternetService'], df['Churn'], normalize='index') * 100\n",
        "internet_churn.plot(kind='bar', stacked=True, colormap='summer', ax=plt.gca())\n",
        "plt.title('Churn Rate by Internet Service', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Internet Service', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 8: Gender distribution\n",
        "plt.subplot(3, 3, 8)\n",
        "gender_churn = pd.crosstab(df['gender'], df['Churn'])\n",
        "gender_churn.plot(kind='bar', stacked=True, colormap='Pastel1', ax=plt.gca())\n",
        "plt.title('Churn Distribution by Gender', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Gender', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 9: Senior citizen impact\n",
        "plt.subplot(3, 3, 9)\n",
        "senior_churn = pd.crosstab(df['SeniorCitizen'], df['Churn'], normalize='index') * 100\n",
        "senior_churn.index = ['Non-Senior', 'Senior']\n",
        "senior_churn.plot(kind='bar', stacked=True, colormap='RdYlBu_r', ax=plt.gca())\n",
        "plt.title('Churn Rate by Senior Citizen Status', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Senior Citizen Status', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "eda_path = os.path.join(DRIVE_PATH, 'eda_visualizations.png')\n",
        "plt.savefig(eda_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n EDA visualizations saved as '{eda_path}'\")\n",
        "\n",
        "# Show EDA insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY EDA INSIGHTS:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Class Imbalance: Significant imbalance (73% No Churn vs 27% Churn)\")\n",
        "print(\"2. Monthly Charges: Churned customers tend to have higher monthly charges\")\n",
        "print(\"3. Contract Type: Month-to-month contracts have highest churn rate\")\n",
        "print(\"4. Tenure: Long-term customers less likely to churn\")\n",
        "print(\"5. Payment Method: Electronic check has highest churn rate\")\n",
        "print(\"6. Internet Service: Fiber optic customers more likely to churn\")\n",
        "print(\"7. Senior Citizens: Slightly higher churn rate among seniors\")\n",
        "\n",
        "print(\"\\n EDA completed!\")"
      ],
      "metadata": {
        "id": "_PiCMBrOsTY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PREPROCESSING & FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "dI0Rrqrit0uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA PREPROCESSING & FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 3.1 Encode categorical variables\n",
        "print(\"\\n3.1 ENCODING CATEGORICAL VARIABLES...\")\n",
        "\n",
        "# Binary encoding for Yes/No columns\n",
        "binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Label encoding for other categoricals\n",
        "label_encoders = {}\n",
        "categorical_cols = ['gender', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                    'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
        "                    'StreamingMovies', 'Contract', 'PaymentMethod']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Target encoding\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "print(f\"   Encoded {len(binary_cols) + len(categorical_cols)} categorical features\")\n",
        "\n",
        "# 3.2 Feature selection\n",
        "print(\"\\n3.2 SELECTING FEATURES...\")\n",
        "# Drop customerID\n",
        "X = df.drop(['customerID', 'Churn'], axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "print(f\"   Features selected: {X.shape[1]}\")\n",
        "print(f\"   Feature names: {X.columns.tolist()}\")\n",
        "\n",
        "# 3.3 Train-test split (BEFORE SMOTE to avoid data leakage)\n",
        "print(\"\\n3.3 TRAIN-TEST SPLIT (80-20)...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"   Training set: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"   Test set:     {X_test.shape} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Check class distribution in train/test\n",
        "print(f\"\\n   Class distribution in Training set:\")\n",
        "train_counts = y_train.value_counts()\n",
        "for val, count in train_counts.items():\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"     {label}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n   Class distribution in Test set:\")\n",
        "test_counts = y_test.value_counts()\n",
        "for val, count in test_counts.items():\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"     {label}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# 3.4 Feature scaling\n",
        "print(\"\\n3.4 SCALING FEATURES...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"   Features scaled using StandardScaler\")\n",
        "\n",
        "print(\"\\n Data preprocessing completed!\")"
      ],
      "metadata": {
        "id": "rmssghj4t2t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HANDLING CLASS IMBALANCE WITH SMOTE"
      ],
      "metadata": {
        "id": "AXX2Akjru7eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HANDLING CLASS IMBALANCE WITH SMOTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define DRIVE_PATH for saving visualizations\n",
        "DRIVE_PATH = './colab_output' # Local directory for demonstration\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# 4.1 Check class distribution BEFORE SMOTE\n",
        "print(\"\\n4.1 CLASS DISTRIBUTION BEFORE SMOTE (Training set):\")\n",
        "unique_before, counts_before = np.unique(y_train, return_counts=True)\n",
        "for val, count in zip(unique_before, counts_before):\n",
        "    percentage = count / len(y_train) * 100\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# 4.2 Apply SMOTE ONLY to training data\n",
        "print(\"\\n4.2 APPLYING SMOTE TO TRAINING DATA...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# 4.3 Check class distribution AFTER SMOTE\n",
        "print(\"4.3 CLASS DISTRIBUTION AFTER SMOTE:\")\n",
        "unique_after, counts_after = np.unique(y_train_bal, return_counts=True)\n",
        "for val, count in zip(unique_after, counts_after):\n",
        "    percentage = count / len(y_train_bal) * 100\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "class_dist_after = {\n",
        "    'No': counts_after[0],\n",
        "    'Yes': counts_after[1],\n",
        "    'No_percent': counts_after[0]/len(y_train_bal)*100,\n",
        "    'Yes_percent': counts_after[1]/len(y_train_bal)*100\n",
        "}\n",
        "\n",
        "# 4.4 Visualize before/after SMOTE\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "labels = ['No Churn', 'Churn']\n",
        "colors = ['lightblue', 'salmon']\n",
        "plt.pie(counts_before, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Class Distribution\\n(Before SMOTE)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(counts_after, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Class Distribution\\n(After SMOTE)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "smote_path = os.path.join(DRIVE_PATH, 'smote_comparison.png')\n",
        "plt.savefig(smote_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n SMOTE comparison visualization saved as '{smote_path}'\")\n",
        "\n",
        "print(\"\\n SMOTE applied successfully. Training data is now balanced.\")"
      ],
      "metadata": {
        "id": "7hO-LU5mvDl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DECISION TREE MODEL"
      ],
      "metadata": {
        "id": "Bth9YghcvGZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DECISION TREE MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 5.1 Hyperparameter tuning with GridSearchCV\n",
        "print(\"\\n5.1 PERFORMING HYPERPARAMETER TUNING...\")\n",
        "print(\"   This may take a few minutes...\")\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "    'min_samples_split': [10, 20, 30, 40],\n",
        "    'min_samples_leaf': [5, 10, 20, 30],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'class_weight': ['balanced'],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    dt_model, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1\n",
        ")\n",
        "grid_search.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "best_dt = grid_search.best_estimator_\n",
        "print(f\"\\n Hyperparameter tuning completed!\")\n",
        "print(f\"   Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"   Best CV F1-score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# 5.2 Train final model\n",
        "print(\"\\n5.2 TRAINING FINAL DECISION TREE...\")\n",
        "best_dt.fit(X_train_bal, y_train_bal)\n",
        "print(\"   Model training completed!\")\n",
        "\n",
        "# ===== ADD FEATURE IMPORTANCE ANALYSIS =====\n",
        "print(\"\\n5.3 FEATURE IMPORTANCE ANALYSIS...\")\n",
        "\n",
        "# Get feature importance\n",
        "dt_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': best_dt.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\n   Top 10 Most Important Features:\")\n",
        "print(dt_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Create horizontal bar chart for top 10 features\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(dt_importance['feature'].head(10)[::-1],\n",
        "         dt_importance['importance'].head(10)[::-1],\n",
        "         color='steelblue')\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.title('Top 10 Feature Importances - Decision Tree',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('decision_tree_feature_importance.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"   Feature importance chart saved as 'decision_tree_feature_importance.png'\")\n",
        "\n",
        "# 5.4 Predictions\n",
        "print(\"\\n5.4 MAKING PREDICTIONS...\")\n",
        "y_pred_dt = best_dt.predict(X_test_scaled)\n",
        "y_pred_proba_dt = best_dt.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# 5.5 Evaluation\n",
        "print(\"\\n5.5 EVALUATING DECISION TREE...\")\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "dt_precision = precision_score(y_test, y_pred_dt)\n",
        "dt_recall = recall_score(y_test, y_pred_dt)\n",
        "dt_f1 = f1_score(y_test, y_pred_dt)\n",
        "dt_auc = roc_auc_score(y_test, y_pred_proba_dt)\n",
        "\n",
        "print(f\"    Accuracy:  {dt_accuracy:.4f}\")\n",
        "print(f\"    Precision: {dt_precision:.4f}\")\n",
        "print(f\"    Recall:    {dt_recall:.4f}\")\n",
        "print(f\"    F1-Score:  {dt_f1:.4f}\")\n",
        "print(f\"    AUC:       {dt_auc:.4f}\")\n",
        "\n",
        "# 5.6 Confusion Matrix\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "print(f\"\\n   Confusion Matrix:\")\n",
        "print(f\"   [[TN: {cm_dt[0,0]}, FP: {cm_dt[0,1]}]\")\n",
        "print(f\"    [FN: {cm_dt[1,0]}, TP: {cm_dt[1,1]}]]\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Decision Tree model completed!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "6xCL5cFRwWDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEURAL NETWORK MODEL"
      ],
      "metadata": {
        "id": "lgFJ9405w9Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NEURAL NETWORK MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 6.1 Model architecture\n",
        "print(\"\\n6.1 BUILDING NEURAL NETWORK ARCHITECTURE...\")\n",
        "nn_model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(X_train_bal.shape[1],)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(\"   Neural Network Architecture:\")\n",
        "nn_model.summary()\n",
        "\n",
        "# 6.2 Compile with Adam optimizer\n",
        "print(\"\\n6.2 COMPILING MODEL...\")\n",
        "nn_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy',\n",
        "             keras.metrics.Precision(name='precision'),\n",
        "             keras.metrics.Recall(name='recall'),\n",
        "             keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "\n",
        "print(\"    Model compiled with Adam optimizer (learning_rate=0.0005)\")\n",
        "\n",
        "# 6.3 Callbacks\n",
        "print(\"\\n6.3 SETTING UP CALLBACKS...\")\n",
        "callbacks_list = [\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# 6.4 Train the model\n",
        "print(\"\\n6.4 TRAINING NEURAL NETWORK...\")\n",
        "print(\"   This may take a few minutes...\")\n",
        "\n",
        "history = nn_model.fit(\n",
        "    X_train_bal, y_train_bal,\n",
        "    validation_split=0.2,\n",
        "    epochs=150,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\n Training completed!\")\n",
        "print(f\"   Final epoch: {len(history.history['loss'])}\")\n",
        "\n",
        "# 6.5 Predictions\n",
        "y_pred_proba_nn = nn_model.predict(X_test_scaled, verbose=0)\n",
        "y_pred_nn = (y_pred_proba_nn > 0.5).astype(int)\n",
        "\n",
        "# 6.6 Evaluation\n",
        "print(\"\\n6.5 EVALUATING NEURAL NETWORK...\")\n",
        "nn_accuracy = accuracy_score(y_test, y_pred_nn)\n",
        "nn_precision = precision_score(y_test, y_pred_nn)\n",
        "nn_recall = recall_score(y_test, y_pred_nn)\n",
        "nn_f1 = f1_score(y_test, y_pred_nn)\n",
        "nn_auc = roc_auc_score(y_test, y_pred_proba_nn)\n",
        "\n",
        "print(f\"   Accuracy:  {nn_accuracy:.4f}\")\n",
        "print(f\"   Precision: {nn_precision:.4f}\")\n",
        "print(f\"   Recall:    {nn_recall:.4f}\")\n",
        "print(f\"   F1-Score:  {nn_f1:.4f}\")\n",
        "print(f\"   AUC:       {nn_auc:.4f}\")\n",
        "\n",
        "# 6.7 Confusion Matrix\n",
        "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
        "print(f\"\\n   Confusion Matrix:\")\n",
        "print(f\"   [[TN: {cm_nn[0,0]}, FP: {cm_nn[0,1]}]\")\n",
        "print(f\"    [FN: {cm_nn[1,0]}, TP: {cm_nn[1,1]}]]\")\n",
        "\n",
        "print(\"\\n Neural Network model completed!\")"
      ],
      "metadata": {
        "id": "dgY-Tgguw-Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL COMPARISON, VISUALIZATION & FINAL OUTPUT"
      ],
      "metadata": {
        "id": "V2Rv9uBwzAs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL COMPARISON AND RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Collect metrics for comprehensive comparison\n",
        "print(\"\\nCollecting model metrics for comparison...\")\n",
        "\n",
        "# Calculate additional metrics for both models\n",
        "# 1. Calculate CV scores for Decision Tree\n",
        "cv_scores_dt = grid_search.cv_results_['mean_test_score']\n",
        "cv_mean_f1_dt = grid_search.best_score_\n",
        "cv_std_f1_dt = cv_scores_dt.std()\n",
        "\n",
        "# 2. Calculate training metrics for Decision Tree\n",
        "y_train_pred_dt = best_dt.predict(X_train_bal)\n",
        "train_accuracy_dt = accuracy_score(y_train_bal, y_train_pred_dt)\n",
        "train_precision_dt = precision_score(y_train_bal, y_train_pred_dt)\n",
        "train_recall_dt = recall_score(y_train_bal, y_train_pred_dt)\n",
        "train_f1_dt = f1_score(y_train_bal, y_train_pred_dt)\n",
        "\n",
        "# Calculate test average precision for Decision Tree\n",
        "from sklearn.metrics import average_precision_score\n",
        "test_avg_precision_dt = average_precision_score(y_test, y_pred_proba_dt)\n",
        "\n",
        "# 3. Calculate training metrics for Neural Network\n",
        "y_train_pred_nn = (nn_model.predict(X_train_bal, verbose=0) > 0.5).astype(int)\n",
        "train_accuracy_nn = history.history['accuracy'][-1] if len(history.history['accuracy']) > 0 else accuracy_score(y_train_bal, y_train_pred_nn)\n",
        "train_precision_nn = history.history['precision'][-1] if 'precision' in history.history else precision_score(y_train_bal, y_train_pred_nn)\n",
        "train_recall_nn = history.history['recall'][-1] if 'recall' in history.history else recall_score(y_train_bal, y_train_pred_nn)\n",
        "train_f1_nn = history.history.get('f1', [f1_score(y_train_bal, y_train_pred_nn)])[-1]\n",
        "\n",
        "# 4. Calculate CV scores for Neural Network (using simple cross-validation)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create a simpler NN for cross-validation (to save time)\n",
        "simple_nn = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.001,\n",
        "    batch_size=32,\n",
        "    max_iter=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform cross-validation on balanced data\n",
        "print(\"   Performing cross-validation for Neural Network...\")\n",
        "cv_scores_nn = cross_val_score(simple_nn, X_train_bal, y_train_bal,\n",
        "                               cv=5, scoring='f1', n_jobs=-1)\n",
        "cv_mean_f1_nn = cv_scores_nn.mean()\n",
        "cv_std_f1_nn = cv_scores_nn.std()\n",
        "\n",
        "# Calculate test average precision for Neural Network\n",
        "test_avg_precision_nn = average_precision_score(y_test, y_pred_proba_nn)\n",
        "\n",
        "# Calculate gaps\n",
        "accuracy_gap_dt = train_accuracy_dt - dt_accuracy\n",
        "f1_gap_dt = train_f1_dt - dt_f1\n",
        "\n",
        "accuracy_gap_nn = train_accuracy_nn - nn_accuracy\n",
        "f1_gap_nn = train_f1_nn - nn_f1\n",
        "\n",
        "# Create comprehensive metrics dictionaries\n",
        "dt_metrics = {\n",
        "    'Model': 'Decision Tree',\n",
        "    'CV Mean F1': cv_mean_f1_dt,\n",
        "    'CV Std F1': cv_std_f1_dt,\n",
        "    'Train Accuracy': train_accuracy_dt,\n",
        "    'Test Accuracy': dt_accuracy,\n",
        "    'Train Precision': train_precision_dt,\n",
        "    'Test Precision': dt_precision,\n",
        "    'Train Recall': train_recall_dt,\n",
        "    'Test Recall': dt_recall,\n",
        "    'Train F1': train_f1_dt,\n",
        "    'Test F1': dt_f1,\n",
        "    'Test AUC-ROC': dt_auc,\n",
        "    'Test Average Precision': test_avg_precision_dt,\n",
        "    'Accuracy Gap': accuracy_gap_dt,\n",
        "    'F1 Gap': f1_gap_dt\n",
        "}\n",
        "\n",
        "nn_metrics = {\n",
        "    'Model': 'Neural Network',\n",
        "    'CV Mean F1': cv_mean_f1_nn,\n",
        "    'CV Std F1': cv_std_f1_nn,\n",
        "    'Train Accuracy': train_accuracy_nn,\n",
        "    'Test Accuracy': nn_accuracy,\n",
        "    'Train Precision': train_precision_nn,\n",
        "    'Test Precision': nn_precision,\n",
        "    'Train Recall': train_recall_nn,\n",
        "    'Test Recall': nn_recall,\n",
        "    'Train F1': train_f1_nn,\n",
        "    'Test F1': nn_f1,\n",
        "    'Test AUC-ROC': nn_auc,\n",
        "    'Test Average Precision': test_avg_precision_nn,\n",
        "    'Accuracy Gap': accuracy_gap_nn,\n",
        "    'F1 Gap': f1_gap_nn\n",
        "}\n",
        "\n",
        "# Create comparison dataframe\n",
        "all_metrics = [dt_metrics, nn_metrics]\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "\n",
        "# Round all numeric columns to 4 decimal places\n",
        "numeric_cols = metrics_df.select_dtypes(include=[np.number]).columns\n",
        "metrics_df[numeric_cols] = metrics_df[numeric_cols].round(4)\n",
        "\n",
        "# 7.1 Performance comparison dataframe\n",
        "print(\"\\nMODEL PERFORMANCE COMPARISON:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Reorder columns to match the desired format\n",
        "column_order = [\n",
        "    'Model', 'CV Mean F1', 'CV Std F1',\n",
        "    'Train Accuracy', 'Test Accuracy',\n",
        "    'Train Precision', 'Test Precision',\n",
        "    'Train Recall', 'Test Recall',\n",
        "    'Train F1', 'Test F1',\n",
        "    'Test AUC-ROC', 'Test Average Precision',\n",
        "    'Accuracy Gap', 'F1 Gap'\n",
        "]\n",
        "\n",
        "metrics_df = metrics_df[column_order]\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# 7.2 Determine best model\n",
        "print(\"\\n7.2 BEST MODEL SELECTION:\")\n",
        "# Find best model based on Test F1-Score\n",
        "best_overall_idx = metrics_df['Test F1'].idxmax()\n",
        "best_model_name = metrics_df.loc[best_overall_idx, 'Model']\n",
        "best_test_f1 = metrics_df.loc[best_overall_idx, 'Test F1']\n",
        "best_test_acc = metrics_df.loc[best_overall_idx, 'Test Accuracy']\n",
        "\n",
        "print(f\"\\n BEST PERFORMING MODEL: {best_model_name}\")\n",
        "print(f\"   Test F1 Score: {best_test_f1:.4f}\")\n",
        "print(f\"   Test Accuracy: {best_test_acc:.4f}\")\n",
        "\n",
        "# Also show which model has better AUC-ROC\n",
        "best_auc_idx = metrics_df['Test AUC-ROC'].idxmax()\n",
        "best_auc_model = metrics_df.loc[best_auc_idx, 'Model']\n",
        "best_auc = metrics_df.loc[best_auc_idx, 'Test AUC-ROC']\n",
        "print(f\"   Best AUC-ROC: {best_auc_model} ({best_auc:.4f})\")\n",
        "\n",
        "# Show which model has better Average Precision\n",
        "best_ap_idx = metrics_df['Test Average Precision'].idxmax()\n",
        "best_ap_model = metrics_df.loc[best_ap_idx, 'Model']\n",
        "best_ap = metrics_df.loc[best_ap_idx, 'Test Average Precision']\n",
        "print(f\"   Best Average Precision: {best_ap_model} ({best_ap:.4f})\")\n",
        "\n",
        "# 7.3 Visualizations - Updated with cleaner layout\n",
        "print(\"\\n7.3 CREATING COMPREHENSIVE VISUALIZATIONS...\")\n",
        "\n",
        "# Main comparison figure\n",
        "fig = plt.figure(figsize=(25, 22))\n",
        "fig.suptitle('TELCO CUSTOMER CHURN PREDICTION - MODEL COMPARISON',\n",
        "             fontsize=18, fontweight='bold', y=1.02)\n",
        "\n",
        "# ========== ROW 1: Training History ==========\n",
        "# Subplot 1: Training history - Accuracy\n",
        "ax1 = plt.subplot(5, 4, 1)\n",
        "ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "ax1.set_title('Neural Network - Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epochs', fontsize=12)\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Training history - Loss\n",
        "ax2 = plt.subplot(5, 4, 2)\n",
        "ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "ax2.set_title('Neural Network - Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epochs', fontsize=12)\n",
        "ax2.set_ylabel('Loss', fontsize=12)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Precision-Recall Tradeoff\n",
        "ax3 = plt.subplot(5, 4, 3)\n",
        "ax3.plot(history.history['precision'], label='Precision', linewidth=2)\n",
        "ax3.plot(history.history['recall'], label='Recall', linewidth=2)\n",
        "ax3.set_title('Precision-Recall Tradeoff (NN)', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('Epochs', fontsize=12)\n",
        "ax3.set_ylabel('Score', fontsize=12)\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "\n",
        "# ========== ROW 2: ROC Curves & Feature Importance ==========\n",
        "# Subplot 5: ROC Curves Comparison\n",
        "ax5 = plt.subplot(5, 4, 5)\n",
        "# DT ROC\n",
        "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)\n",
        "ax5.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC={dt_auc:.3f})',\n",
        "         linewidth=2.5, color='blue', alpha=0.8)\n",
        "# NN ROC\n",
        "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_pred_proba_nn)\n",
        "ax5.plot(fpr_nn, tpr_nn, label=f'Neural Network (AUC={nn_auc:.3f})',\n",
        "         linewidth=2.5, color='green', alpha=0.8)\n",
        "ax5.plot([0, 1], [0, 1], 'k--', label='Random Classifier', alpha=0.5)\n",
        "ax5.set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
        "ax5.set_xlabel('False Positive Rate', fontsize=12)\n",
        "ax5.set_ylabel('True Positive Rate', fontsize=12)\n",
        "ax5.legend()\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 6: Feature Importance\n",
        "ax6 = plt.subplot(5, 4, 6)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': best_dt.feature_importances_\n",
        "}).sort_values('importance', ascending=False).head(10)\n",
        "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis', ax=ax6)\n",
        "ax6.set_title('Top 10 Feature Importance (Decision Tree)', fontsize=14, fontweight='bold')\n",
        "ax6.set_xlabel('Importance Score', fontsize=12)\n",
        "ax6.set_ylabel('Feature', fontsize=12)\n",
        "\n",
        "# ========== ROW 3: Confusion Matrices ==========\n",
        "# Subplot 7: Decision Tree Confusion Matrix\n",
        "ax7 = plt.subplot(5, 4, 7)\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', ax=ax7,\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "ax7.set_title(f'Decision Tree\\nF1 Score: {dt_f1:.3f}', fontsize=14, fontweight='bold')\n",
        "ax7.set_ylabel('Actual', fontsize=12)\n",
        "ax7.set_xlabel('Predicted', fontsize=12)\n",
        "\n",
        "# Subplot 8: Neural Network Confusion Matrix\n",
        "ax8 = plt.subplot(5, 4, 8)\n",
        "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Greens', ax=ax8,\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "ax8.set_title(f'Neural Network\\nF1 Score: {nn_f1:.3f}', fontsize=14, fontweight='bold')\n",
        "ax8.set_ylabel('Actual', fontsize=12)\n",
        "ax8.set_xlabel('Predicted', fontsize=12)\n",
        "\n",
        "# ========== ROW 4: Metrics Comparison ==========\n",
        "# Subplot 9: Performance Metrics Bar Chart (Enhanced)\n",
        "ax9 = plt.subplot(5, 4, (9, 10))  # Span 2 columns\n",
        "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1', 'Test AUC-ROC']\n",
        "dt_scores = [dt_accuracy, dt_precision, dt_recall, dt_f1, dt_auc]\n",
        "nn_scores = [nn_accuracy, nn_precision, nn_recall, nn_f1, nn_auc]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "bars1 = ax9.bar(x - width/2, dt_scores, width, label='Decision Tree',\n",
        "                color='skyblue', edgecolor='black')\n",
        "bars2 = ax9.bar(x + width/2, nn_scores, width, label='Neural Network',\n",
        "                color='lightgreen', edgecolor='black')\n",
        "\n",
        "ax9.set_title('Model Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "ax9.set_xlabel('Metrics', fontsize=12)\n",
        "ax9.set_ylabel('Score', fontsize=12)\n",
        "ax9.set_xticks(x)\n",
        "ax9.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "ax9.legend()\n",
        "ax9.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax9.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Subplot 10: Accuracy Gap Comparison\n",
        "ax10 = plt.subplot(5, 4, 11)\n",
        "gap_metrics = ['Accuracy Gap']\n",
        "dt_gaps = [accuracy_gap_dt]\n",
        "nn_gaps = [accuracy_gap_nn]\n",
        "x_gap = np.arange(len(gap_metrics))\n",
        "bars_gap1 = ax10.bar(x_gap - width/2, dt_gaps, width, label='Decision Tree',\n",
        "                     color='lightcoral', edgecolor='black')\n",
        "bars_gap2 = ax10.bar(x_gap + width/2, nn_gaps, width, label='Neural Network',\n",
        "                     color='gold', edgecolor='black')\n",
        "\n",
        "ax10.set_title('Accuracy Gap (Train-Test)', fontsize=14, fontweight='bold')\n",
        "ax10.set_ylabel('Gap Score', fontsize=12)\n",
        "ax10.set_xticks(x_gap)\n",
        "ax10.set_xticklabels(['Gap'], fontsize=12)\n",
        "ax10.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "ax10.axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Overfit Threshold')\n",
        "ax10.axhline(y=-0.05, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
        "ax10.legend()\n",
        "ax10.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels for gap bars\n",
        "for bar, value in zip(bars_gap1 + bars_gap2, dt_gaps + nn_gaps):\n",
        "    height = bar.get_height()\n",
        "    ax10.text(bar.get_x() + bar.get_width()/2., height + (0.001 if height >= 0 else -0.015),\n",
        "              f'{height:.3f}', ha='center', va='bottom' if height >= 0 else 'top', fontsize=9)\n",
        "\n",
        "# Subplot 11: CV F1 Scores Comparison\n",
        "ax11 = plt.subplot(5, 4, 12)\n",
        "models = ['Decision Tree', 'Neural Network']\n",
        "cv_means = [cv_mean_f1_dt, cv_mean_f1_nn]\n",
        "cv_stds = [cv_std_f1_dt, cv_std_f1_nn]\n",
        "\n",
        "x_pos = np.arange(len(models))\n",
        "bars = ax11.bar(x_pos, cv_means, yerr=cv_stds, capsize=5,\n",
        "                color=['skyblue', 'lightgreen'], edgecolor='black')\n",
        "ax11.set_title('Cross-Validation F1 Scores', fontsize=14, fontweight='bold')\n",
        "ax11.set_ylabel('F1 Score', fontsize=12)\n",
        "ax11.set_xticks(x_pos)\n",
        "ax11.set_xticklabels(models, fontsize=12)\n",
        "ax11.set_ylim([0, 1])\n",
        "ax11.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, mean, std in zip(bars, cv_means, cv_stds):\n",
        "    height = bar.get_height()\n",
        "    ax11.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "              f'{mean:.3f} Â± {std:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# ========== ROW 5: Decision Tree Visualization ==========\n",
        "# Subplot 12: Decision Tree Visualization\n",
        "ax12 = plt.subplot(5, 4, (13, 16))  # Span 4 columns\n",
        "plot_tree(best_dt, max_depth=3, feature_names=X.columns,\n",
        "          class_names=['No Churn', 'Churn'], filled=True, rounded=True,\n",
        "          proportion=True, fontsize=8, ax=ax12)\n",
        "ax12.set_title('Decision Tree Visualization (First 3 Levels)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "comparison_path = os.path.join(DRIVE_PATH, 'model_comparison_results.png')\n",
        "plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n Model comparison visualizations saved as '{comparison_path}'\")\n",
        "\n",
        "# ============================================================================\n",
        "# DETAILED CLASSIFICATION REPORTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n1. DECISION TREE CLASSIFICATION REPORT:\")\n",
        "print(\"-\" * 60)\n",
        "print(classification_report(y_test, y_pred_dt, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "print(\"\\n2. NEURAL NETWORK CLASSIFICATION REPORT:\")\n",
        "print(\"-\" * 60)\n",
        "print(classification_report(y_test, y_pred_nn, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "# Detailed confusion matrix analysis\n",
        "print(\"\\n3. CONFUSION MATRIX ANALYSIS:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for pred, model_name, cm in [(y_pred_dt, \"Decision Tree\", cm_dt),\n",
        "                             (y_pred_nn, \"Neural Network\", cm_nn)]:\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  True Negatives (TN): {tn:4d} - Correctly predicted 'No Churn'\")\n",
        "    print(f\"  False Positives (FP): {fp:4d} - Predicted 'Churn' but actual 'No Churn'\")\n",
        "    print(f\"  False Negatives (FN): {fn:4d} - Predicted 'No Churn' but actual 'Churn'\")\n",
        "    print(f\"  True Positives (TP): {tp:4d} - Correctly predicted 'Churn'\")\n",
        "\n",
        "    # Calculate rates\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 8.1 Save models\n",
        "print(\"\\n8.1 SAVING MODELS...\")\n",
        "\n",
        "# Save Decision Tree\n",
        "dt_path = os.path.join(DRIVE_PATH, 'decision_tree_model.pkl')\n",
        "joblib.dump(best_dt, dt_path)\n",
        "print(f\"   Decision Tree saved as '{dt_path}'\")\n",
        "\n",
        "# Save Neural Network\n",
        "nn_path = os.path.join(DRIVE_PATH, 'neural_network_model.h5')\n",
        "nn_model.save(nn_path)\n",
        "print(f\"   Neural Network saved as '{nn_path}'\")\n",
        "\n",
        "# Save scaler\n",
        "scaler_path = os.path.join(DRIVE_PATH, 'scaler.pkl')\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"   Scaler saved as '{scaler_path}'\")\n",
        "\n",
        "# Save label encoders\n",
        "encoders_path = os.path.join(DRIVE_PATH, 'label_encoders.pkl')\n",
        "joblib.dump(label_encoders, encoders_path)\n",
        "print(f\"   Label encoders saved as '{encoders_path}'\")\n",
        "\n",
        "# 8.2 Save results\n",
        "print(\"\\n8.2 SAVING RESULTS...\")\n",
        "\n",
        "# Save performance comparison\n",
        "results_csv_path = os.path.join(DRIVE_PATH, 'model_comparison_results.csv')\n",
        "metrics_df.to_csv(results_csv_path, index=False)\n",
        "print(f\"   Results saved as '{results_csv_path}'\")\n",
        "\n",
        "# Save detailed classification reports\n",
        "report_path = os.path.join(DRIVE_PATH, 'detailed_reports.txt')\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(\"=\"*60 + \"\\n\")\n",
        "    f.write(\"TELCO CUSTOMER CHURN PREDICTION - DETAILED REPORTS\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"BEST MODEL SELECTION:\\n\")\n",
        "    f.write(\"=\"*40 + \"\\n\")\n",
        "    f.write(f\" BEST PERFORMING MODEL: {best_model_name}\\n\")\n",
        "    f.write(f\"   Test F1 Score: {best_test_f1:.4f}\\n\")\n",
        "    f.write(f\"   Test Accuracy: {best_test_acc:.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"MODEL PERFORMANCE COMPARISON:\\n\")\n",
        "    f.write(\"=\"*40 + \"\\n\")\n",
        "    f.write(metrics_df.to_string(index=False) + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"DECISION TREE DETAILS:\\n\")\n",
        "    f.write(\"=\"*40 + \"\\n\")\n",
        "    f.write(f\"Best parameters: {grid_search.best_params_}\\n\")\n",
        "    f.write(f\"Best CV F1-score: {grid_search.best_score_:.4f}\\n\")\n",
        "    f.write(f\"CV F1 std: {cv_std_f1_dt:.4f}\\n\")\n",
        "    f.write(classification_report(y_test, y_pred_dt, target_names=['No Churn', 'Churn']))\n",
        "    f.write(f\"Confusion Matrix:\\n{cm_dt}\\n\\n\")\n",
        "\n",
        "    f.write(\"NEURAL NETWORK DETAILS:\\n\")\n",
        "    f.write(\"=\"*40 + \"\\n\")\n",
        "    f.write(f\"CV Mean F1: {cv_mean_f1_nn:.4f}\\n\")\n",
        "    f.write(f\"CV F1 std: {cv_std_f1_nn:.4f}\\n\")\n",
        "    f.write(f\"Final training accuracy: {train_accuracy_nn:.4f}\\n\")\n",
        "    f.write(f\"Number of epochs trained: {len(history.history['loss'])}\\n\")\n",
        "    f.write(classification_report(y_test, y_pred_nn, target_names=['No Churn', 'Churn']))\n",
        "    f.write(f\"Confusion Matrix:\\n{cm_nn}\\n\\n\")\n",
        "\n",
        "print(f\"\\n Detailed report saved as '{report_path}'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" TASK 2 COMPLETE: Both models implemented, tuned, and compared\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "7VkbE2n-0Ed6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
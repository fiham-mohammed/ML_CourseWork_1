{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gkFoj9zX2SdGOphlL5a0AtGcBClPISoL",
      "authorship_tag": "ABX9TyNEyDTP6EHFwjEPMzh5UcsH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORT AND  INSTALLATION"
      ],
      "metadata": {
        "id": "teM4VczdWE8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install imbalanced-learn -q\n",
        "!pip install tensorflow -q\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, confusion_matrix,\n",
        "                             classification_report, roc_curve)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import joblib\n",
        "import os\n",
        "from google.colab import drive, files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ],
      "metadata": {
        "id": "yirxqMdKWEMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA LOADING & INITIAL EXPLORATION"
      ],
      "metadata": {
        "id": "FOr8XlUyrifm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TELCO CUSTOMER CHURN PREDICTION - COURSEWORK IMPLEMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download dataset from Kaggle (alternative method)\n",
        "print(\"\\n1. DOWNLOADING DATASET...\")\n",
        "\n",
        "# Method 1: Try loading from Google Drive path first\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Telco-Customer-Churn.csv\")\n",
        "    print(f\"   Dataset loaded successfully from Google Drive!\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Columns: {df.columns.tolist()}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"   File not found at Google Drive path. Trying alternative method...\")\n",
        "\n",
        "    # Method 2: Upload from local\n",
        "    print(\"\\n   Please upload the 'Telco-Customer-Churn.csv' file when prompted\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the filename\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        print(f\"   Uploaded file: {filename}\")\n",
        "\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(filename)\n",
        "        print(f\"   Dataset loaded successfully from uploaded file!\")\n",
        "        print(f\"   Shape: {df.shape}\")\n",
        "        print(f\"   Columns: {df.columns.tolist()}\")\n",
        "    else:\n",
        "        print(\"    No file uploaded. Please try again.\")\n",
        "        raise FileNotFoundError(\"Telco-Customer-Churn.csv file not found\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n Data loading completed!\")"
      ],
      "metadata": {
        "id": "GhCIEW-Nr5Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXPLORATORY DATA ANALYSIS (EDA)"
      ],
      "metadata": {
        "id": "yo57YIxbsRlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPLORATORY DATA ANALYSIS (EDA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define DRIVE_PATH for saving visualizations\n",
        "DRIVE_PATH = './colab_output'\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# 2.1 Basic information\n",
        "print(\"\\n2.1 DATASET INFORMATION:\")\n",
        "print(f\"   Total samples: {len(df)}\")\n",
        "print(f\"   Total features: {len(df.columns)}\")\n",
        "\n",
        "# Data types\n",
        "print(\"\\n2.2 DATA TYPES:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Missing values\n",
        "print(\"\\n2.3 MISSING VALUES:\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0])\n",
        "\n",
        "# Handle missing values in TotalCharges\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "# 2.4 Class Distribution - BEFORE BALANCING\n",
        "print(\"\\n2.4 CLASS DISTRIBUTION (BEFORE BALANCING):\")\n",
        "churn_counts = df['Churn'].value_counts()\n",
        "churn_percent = df['Churn'].value_counts(normalize=True) * 100\n",
        "print(f\"   No Churn:  {churn_counts['No']} ({churn_percent['No']:.1f}%)\")\n",
        "print(f\"   Churn:     {churn_counts['Yes']} ({churn_percent['Yes']:.1f}%)\")\n",
        "print(f\"   IMBALANCE RATIO: {churn_counts['No']/churn_counts['Yes']:.2f}:1\")\n",
        "\n",
        "# Save for report\n",
        "class_dist_before = {\n",
        "    'No': churn_counts['No'],\n",
        "    'Yes': churn_counts['Yes'],\n",
        "    'No_percent': churn_percent['No'],\n",
        "    'Yes_percent': churn_percent['Yes']\n",
        "}\n",
        "\n",
        "# 2.5 Numerical features statistics\n",
        "print(\"\\n2.5 NUMERICAL FEATURES STATISTICS:\")\n",
        "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "print(df[numerical_cols].describe())\n",
        "\n",
        "# 2.6 Visualizations\n",
        "print(\"\\n2.6 CREATING EDA VISUALIZATIONS...\")\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Subplot 1: Churn distribution\n",
        "plt.subplot(3, 3, 1)\n",
        "sns.countplot(data=df, x='Churn', palette='Set2')\n",
        "plt.title('Churn Distribution (Original)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Churn Status', fontsize=12)\n",
        "\n",
        "# Add percentage labels\n",
        "total = len(df)\n",
        "for p in plt.gca().patches:\n",
        "    height = p.get_height()\n",
        "    plt.gca().text(p.get_x() + p.get_width()/2., height + 30,\n",
        "                  f'{height/total*100:.1f}%', ha='center', fontsize=11)\n",
        "\n",
        "# Subplot 2: Tenure distribution\n",
        "plt.subplot(3, 3, 2)\n",
        "sns.histplot(df['tenure'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Customer Tenure Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Tenure (months)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "\n",
        "# Subplot 3: Monthly charges vs Churn\n",
        "plt.subplot(3, 3, 3)\n",
        "sns.boxplot(data=df, x='Churn', y='MonthlyCharges', palette='Set2')\n",
        "plt.title('Monthly Charges by Churn Status', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Churn Status', fontsize=12)\n",
        "plt.ylabel('Monthly Charges ($)', fontsize=12)\n",
        "\n",
        "# Subplot 4: Contract type impact\n",
        "plt.subplot(3, 3, 4)\n",
        "contract_churn = pd.crosstab(df['Contract'], df['Churn'], normalize='index') * 100\n",
        "contract_churn.plot(kind='bar', stacked=True, colormap='coolwarm', ax=plt.gca())\n",
        "plt.title('Churn Rate by Contract Type', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Contract Type', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 5: Correlation heatmap\n",
        "plt.subplot(3, 3, 5)\n",
        "corr_matrix = df[numerical_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.2f', cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Numerical Features Correlation', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Subplot 6: Payment method impact\n",
        "plt.subplot(3, 3, 6)\n",
        "payment_churn = pd.crosstab(df['PaymentMethod'], df['Churn'], normalize='index') * 100\n",
        "payment_churn.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())\n",
        "plt.title('Churn Rate by Payment Method', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Payment Method', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 7: Internet service impact\n",
        "plt.subplot(3, 3, 7)\n",
        "internet_churn = pd.crosstab(df['InternetService'], df['Churn'], normalize='index') * 100\n",
        "internet_churn.plot(kind='bar', stacked=True, colormap='summer', ax=plt.gca())\n",
        "plt.title('Churn Rate by Internet Service', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Internet Service', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 8: Gender distribution\n",
        "plt.subplot(3, 3, 8)\n",
        "gender_churn = pd.crosstab(df['gender'], df['Churn'])\n",
        "gender_churn.plot(kind='bar', stacked=True, colormap='Pastel1', ax=plt.gca())\n",
        "plt.title('Churn Distribution by Gender', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Gender', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Subplot 9: Senior citizen impact\n",
        "plt.subplot(3, 3, 9)\n",
        "senior_churn = pd.crosstab(df['SeniorCitizen'], df['Churn'], normalize='index') * 100\n",
        "senior_churn.index = ['Non-Senior', 'Senior']\n",
        "senior_churn.plot(kind='bar', stacked=True, colormap='RdYlBu_r', ax=plt.gca())\n",
        "plt.title('Churn Rate by Senior Citizen Status', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Percentage (%)', fontsize=12)\n",
        "plt.xlabel('Senior Citizen Status', fontsize=12)\n",
        "plt.legend(title='Churn', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "eda_path = os.path.join(DRIVE_PATH, 'eda_visualizations.png')\n",
        "plt.savefig(eda_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n EDA visualizations saved as '{eda_path}'\")\n",
        "\n",
        "# Show EDA insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY EDA INSIGHTS:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Class Imbalance: Significant imbalance (73% No Churn vs 27% Churn)\")\n",
        "print(\"2. Monthly Charges: Churned customers tend to have higher monthly charges\")\n",
        "print(\"3. Contract Type: Month-to-month contracts have highest churn rate\")\n",
        "print(\"4. Tenure: Long-term customers less likely to churn\")\n",
        "print(\"5. Payment Method: Electronic check has highest churn rate\")\n",
        "print(\"6. Internet Service: Fiber optic customers more likely to churn\")\n",
        "print(\"7. Senior Citizens: Slightly higher churn rate among seniors\")\n",
        "\n",
        "print(\"\\n EDA completed!\")"
      ],
      "metadata": {
        "id": "_PiCMBrOsTY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PREPROCESSING & FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "dI0Rrqrit0uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA PREPROCESSING & FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 3.1 Encode categorical variables\n",
        "print(\"\\n3.1 ENCODING CATEGORICAL VARIABLES...\")\n",
        "\n",
        "# Binary encoding for Yes/No columns\n",
        "binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Label encoding for other categoricals\n",
        "label_encoders = {}\n",
        "categorical_cols = ['gender', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                    'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
        "                    'StreamingMovies', 'Contract', 'PaymentMethod']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Target encoding\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "print(f\"   Encoded {len(binary_cols) + len(categorical_cols)} categorical features\")\n",
        "\n",
        "# 3.2 Feature selection\n",
        "print(\"\\n3.2 SELECTING FEATURES...\")\n",
        "# Drop customerID\n",
        "X = df.drop(['customerID', 'Churn'], axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "print(f\"   Features selected: {X.shape[1]}\")\n",
        "print(f\"   Feature names: {X.columns.tolist()}\")\n",
        "\n",
        "# 3.3 Train-test split (BEFORE SMOTE to avoid data leakage)\n",
        "print(\"\\n3.3 TRAIN-TEST SPLIT (80-20)...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"   Training set: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"   Test set:     {X_test.shape} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Check class distribution in train/test\n",
        "print(f\"\\n   Class distribution in Training set:\")\n",
        "train_counts = y_train.value_counts()\n",
        "for val, count in train_counts.items():\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"     {label}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n   Class distribution in Test set:\")\n",
        "test_counts = y_test.value_counts()\n",
        "for val, count in test_counts.items():\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"     {label}: {count} ({count/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# 3.4 Feature scaling\n",
        "print(\"\\n3.4 SCALING FEATURES...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"   Features scaled using StandardScaler\")\n",
        "\n",
        "print(\"\\n Data preprocessing completed!\")"
      ],
      "metadata": {
        "id": "rmssghj4t2t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HANDLING CLASS IMBALANCE WITH SMOTE"
      ],
      "metadata": {
        "id": "AXX2Akjru7eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HANDLING CLASS IMBALANCE WITH SMOTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define DRIVE_PATH for saving visualizations\n",
        "DRIVE_PATH = './colab_output' # Local directory for demonstration\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# 4.1 Check class distribution BEFORE SMOTE\n",
        "print(\"\\n4.1 CLASS DISTRIBUTION BEFORE SMOTE (Training set):\")\n",
        "unique_before, counts_before = np.unique(y_train, return_counts=True)\n",
        "for val, count in zip(unique_before, counts_before):\n",
        "    percentage = count / len(y_train) * 100\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# 4.2 Apply SMOTE ONLY to training data\n",
        "print(\"\\n4.2 APPLYING SMOTE TO TRAINING DATA...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# 4.3 Check class distribution AFTER SMOTE\n",
        "print(\"4.3 CLASS DISTRIBUTION AFTER SMOTE:\")\n",
        "unique_after, counts_after = np.unique(y_train_bal, return_counts=True)\n",
        "for val, count in zip(unique_after, counts_after):\n",
        "    percentage = count / len(y_train_bal) * 100\n",
        "    label = \"Churn\" if val == 1 else \"No Churn\"\n",
        "    print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "class_dist_after = {\n",
        "    'No': counts_after[0],\n",
        "    'Yes': counts_after[1],\n",
        "    'No_percent': counts_after[0]/len(y_train_bal)*100,\n",
        "    'Yes_percent': counts_after[1]/len(y_train_bal)*100\n",
        "}\n",
        "\n",
        "# 4.4 Visualize before/after SMOTE\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "labels = ['No Churn', 'Churn']\n",
        "colors = ['lightblue', 'salmon']\n",
        "plt.pie(counts_before, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Class Distribution\\n(Before SMOTE)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie(counts_after, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Class Distribution\\n(After SMOTE)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "smote_path = os.path.join(DRIVE_PATH, 'smote_comparison.png')\n",
        "plt.savefig(smote_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n✅ SMOTE comparison visualization saved as '{smote_path}'\")\n",
        "\n",
        "print(\"\\n✅ SMOTE applied successfully. Training data is now balanced.\")"
      ],
      "metadata": {
        "id": "7hO-LU5mvDl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DECISION TREE MODEL"
      ],
      "metadata": {
        "id": "Bth9YghcvGZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DECISION TREE MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 5.1 Hyperparameter tuning with GridSearchCV\n",
        "print(\"\\n5.1 PERFORMING HYPERPARAMETER TUNING...\")\n",
        "print(\"   This may take a few minutes...\")\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "    'min_samples_split': [10, 20, 30, 40],\n",
        "    'min_samples_leaf': [5, 10, 20, 30],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'class_weight': ['balanced'],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    dt_model, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1\n",
        ")\n",
        "grid_search.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "best_dt = grid_search.best_estimator_\n",
        "print(f\"\\n Hyperparameter tuning completed!\")\n",
        "print(f\"   Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"   Best CV F1-score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# 5.2 Train final model\n",
        "print(\"\\n5.2 TRAINING FINAL DECISION TREE...\")\n",
        "best_dt.fit(X_train_bal, y_train_bal)\n",
        "print(\"   Model training completed!\")\n",
        "\n",
        "# ===== ADD FEATURE IMPORTANCE ANALYSIS =====\n",
        "print(\"\\n5.3 FEATURE IMPORTANCE ANALYSIS...\")\n",
        "\n",
        "# Get feature importance\n",
        "dt_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': best_dt.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\n   Top 10 Most Important Features:\")\n",
        "print(dt_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Create horizontal bar chart for top 10 features\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(dt_importance['feature'].head(10)[::-1],\n",
        "         dt_importance['importance'].head(10)[::-1],\n",
        "         color='steelblue')\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.title('Top 10 Feature Importances - Decision Tree',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('decision_tree_feature_importance.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"   Feature importance chart saved as 'decision_tree_feature_importance.png'\")\n",
        "\n",
        "# 5.4 Predictions\n",
        "print(\"\\n5.4 MAKING PREDICTIONS...\")\n",
        "y_pred_dt = best_dt.predict(X_test_scaled)\n",
        "y_pred_proba_dt = best_dt.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# 5.5 Evaluation\n",
        "print(\"\\n5.5 EVALUATING DECISION TREE...\")\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "dt_precision = precision_score(y_test, y_pred_dt)\n",
        "dt_recall = recall_score(y_test, y_pred_dt)\n",
        "dt_f1 = f1_score(y_test, y_pred_dt)\n",
        "dt_auc = roc_auc_score(y_test, y_pred_proba_dt)\n",
        "\n",
        "print(f\"    Accuracy:  {dt_accuracy:.4f}\")\n",
        "print(f\"    Precision: {dt_precision:.4f}\")\n",
        "print(f\"    Recall:    {dt_recall:.4f}\")\n",
        "print(f\"    F1-Score:  {dt_f1:.4f}\")\n",
        "print(f\"    AUC:       {dt_auc:.4f}\")\n",
        "\n",
        "# 5.6 Confusion Matrix\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "print(f\"\\n   Confusion Matrix:\")\n",
        "print(f\"   [[TN: {cm_dt[0,0]}, FP: {cm_dt[0,1]}]\")\n",
        "print(f\"    [FN: {cm_dt[1,0]}, TP: {cm_dt[1,1]}]]\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Decision Tree model completed!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "6xCL5cFRwWDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEURAL NETWORK MODEL"
      ],
      "metadata": {
        "id": "lgFJ9405w9Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgY-Tgguw-Zi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}